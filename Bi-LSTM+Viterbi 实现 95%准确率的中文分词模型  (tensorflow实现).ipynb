{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "py3 bi-lstm viterbi.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "r_WQHaeD8zKe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "操作系统：Linux\n",
        "\n",
        "Python版本: 3.6.7\n",
        "\n",
        "tensorflow版本：'1.13.0-rc1'\n",
        "\n",
        "参与者 github： \n",
        "1. [zxxwin](https://github.com/zxxwin?tab=repositories)    \n",
        "2. [bubblezhong](https://github.com/bubblezhong?tab=repositories)"
      ]
    },
    {
      "metadata": {
        "id": "kwz9fx7KJkh_",
        "colab_type": "code",
        "outputId": "49298f09-e705-4d4d-b5a2-dc77d9f2422c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://spaces.ac.cn/usr/uploads/2016/10/1372394625.zip\n",
        "!unzip 1372394625.zip\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-17 04:49:13--  https://spaces.ac.cn/usr/uploads/2016/10/1372394625.zip\n",
            "Resolving spaces.ac.cn (spaces.ac.cn)... 114.215.107.121\n",
            "Connecting to spaces.ac.cn (spaces.ac.cn)|114.215.107.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6386880 (6.1M) [application/zip]\n",
            "Saving to: ‘1372394625.zip.1’\n",
            "\n",
            "1372394625.zip.1    100%[===================>]   6.09M  3.60MB/s    in 1.7s    \n",
            "\n",
            "2019-02-17 04:49:16 (3.60 MB/s) - ‘1372394625.zip.1’ saved [6386880/6386880]\n",
            "\n",
            "Archive:  1372394625.zip\n",
            "replace msr_train.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: msr_train.txt           \n",
            "1372394625.zip\t1372394625.zip.1  ckpt\tdata  msr_train.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZL765GdpJtEC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tCdQGcFRJ71P",
        "colab_type": "code",
        "outputId": "67a8bb5f-a969-4d70-86d2-ceafe257ce2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "# 以字符串的形式读入所有数据\n",
        "with open('msr_train.txt', 'rb') as msr_data:\n",
        "    texts = msr_data.read().decode('gbk')\n",
        "sentences = texts.split('\\r\\n')  # 根据换行切分\n",
        "\n",
        "\n",
        "# 将不规范的内容（如每行的开头）去掉\n",
        "def clean(s): \n",
        "    if '“/s' not in s: \n",
        "        return s.replace(' ”/s', '')\n",
        "    elif '”/s' not in s:\n",
        "        return s.replace('“/s ', '')\n",
        "    elif '‘/s' not in s:\n",
        "        return s.replace(' ’/s', '')\n",
        "    elif '’/s' not in s:\n",
        "        return s.replace('‘/s ', '')\n",
        "    else:\n",
        "        return s\n",
        "# 把所有的句子拼接起来\n",
        "texts = ''.join(map(clean, sentences)) \n",
        "print('Length of texts is %d' % len(texts))\n",
        "print('Example of texts: \\n', texts[:300])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of texts is 20247877\n",
            "Example of texts: \n",
            "  人/b  们/e  常/s  说/s  生/b  活/e  是/s  一/s  部/s  教/b  科/m  书/e  ，/s  而/s  血/s  与/s  火/s  的/s  战/b  争/e  更/s  是/s  不/b  可/m  多/m  得/e  的/s  教/b  科/m  书/e  ，/s  她/s  确/b  实/e  是/s  名/b  副/m  其/m  实/e  的/s  ‘/s  我/s  的/s  大/b  学/e  ’/s  。/s   心/s  静/s  渐/s  知/s  春/s  似/s  海/s  ，/s  花/s  深/s  每/s  觉/s  影/s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O8M7bbGGKpLC",
        "colab_type": "code",
        "outputId": "dd59a015-b8a4-41f7-ea0e-d9a5ee6a00f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "cell_type": "code",
      "source": [
        "# 重新以标点来划分，标点的形式如： ，/s      \"/e      \"/s\n",
        "sentences = re.split('[，。！？、‘’“”]/[bems]', texts)\n",
        "print('Sentences number:', len(sentences))\n",
        "print('Sentence Example:\\n', sentences[0])\n",
        "print('Sentence Example:\\n', sentences[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentences number: 331739\n",
            "Sentence Example:\n",
            "  人/b  们/e  常/s  说/s  生/b  活/e  是/s  一/s  部/s  教/b  科/m  书/e  \n",
            "Sentence Example:\n",
            "   而/s  血/s  与/s  火/s  的/s  战/b  争/e  更/s  是/s  不/b  可/m  多/m  得/e  的/s  教/b  科/m  书/e  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J51PP46HP8Ku",
        "colab_type": "code",
        "outputId": "55330bc9-8f40-4ef2-eca8-92d3595c08a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "# 将一句话的文字和对应的标签分离\n",
        "def get_words_labels(sentence):\n",
        "  # sentence 可能是空格组成的字符串，因此返回的 words_labels 可能为空数组\n",
        "  words_labels = re.findall('(.)/(.)', sentence)\n",
        "  if words_labels:\n",
        "    words_labels = np.asarray(words_labels)\n",
        "    words = words_labels[:,0]\n",
        "    labels = words_labels[:, 1]\n",
        "    return words, labels\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "sentence_words = []\n",
        "sentence_labels = []\n",
        "\n",
        "# Tqdm 可以在 Python 长循环中添加一个进度提示信息，用户只需要封装任意的迭代器 tqdm(iterator)。\n",
        "for sentence in tqdm(iter(sentences)):\n",
        "    words_labels = get_words_labels(sentence)\n",
        "    # 保证返回的 words_labels 非空 \n",
        "    if words_labels:\n",
        "      sentence_words.append(words_labels[0])\n",
        "      sentence_labels.append(words_labels[1])\n",
        "\n",
        "\n",
        "print(\"sentence_words 长度：\", len(sentence_words))\n",
        "print(\"sentence_words示例：\")\n",
        "print(sentence_words[1])\n",
        "print(\"sentence_labels示例：\")\n",
        "print(sentence_labels[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "331739it [00:04, 82584.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "sentence_words 长度： 321533\n",
            "sentence_words示例：\n",
            "['而' '血' '与' '火' '的' '战' '争' '更' '是' '不' '可' '多' '得' '的' '教' '科' '书']\n",
            "sentence_labels示例：\n",
            "['s' 's' 's' 's' 's' 'b' 'e' 's' 's' 'b' 'm' 'm' 'e' 's' 'b' 'm' 'e']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ns7PuweW9ajQ",
        "colab_type": "code",
        "outputId": "cf79af7c-8e40-4907-cb7d-d7aa7f5b65a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "df_data = pd.DataFrame(index = range(len(sentence_words)))\n",
        "df_data[\"sentence_words\"] = sentence_words\n",
        "df_data[\"sentence_labels\"] = sentence_labels\n",
        "sentence_length = list(map(lambda sentence: len(sentence), sentence_words))\n",
        "df_data[\"sentence_length\"] = sentence_length\n",
        "df_data.head(5)\n",
        "\n",
        "# 将所有句子的字全部合并到数组all_words中\n",
        "all_words = []\n",
        "for sentence_word in tqdm(iter(sentence_words)):\n",
        "  all_words.extend(sentence_word)\n",
        "\n",
        "pd_all_words = pd.Series(all_words)\n",
        "# 对每个值进行计数并且排序\n",
        "pd_all_words_count = pd_all_words.value_counts()\n",
        "# pd_all_words_count.head(5)\n",
        "\n",
        "# 得到pd_all_words_count的索引，也就得到了未重复的字的序列，并将其作为字典\n",
        "dict_words = pd_all_words_count.index\n",
        "# 得到与dict_words长度相同的序列，作为id。 \n",
        "# 从 1 开始是因为我们用 0 来填充不满32个字符的空位\n",
        "dict_words_ids = range(1, len(dict_words) + 1)\n",
        "\n",
        "# x 用来填充不满32个字符的空位\n",
        "word_labels = [ 'x', 's', 'b', 'm', 'e']\n",
        "word_labels_ids = range(len(word_labels))\n",
        "\n",
        "# 以文字为索引，方便得到文字的id\n",
        "word2id = pd.Series(dict_words_ids, index = dict_words)\n",
        "# 以id为索引，方便得到id对应的文字\n",
        "id2word = pd.Series(dict_words, index = dict_words_ids)\n",
        "# 对 labels 进行同样处理\n",
        "label2id = pd.Series(word_labels_ids, index = word_labels)\n",
        "id2label = pd.Series(word_labels, index = word_labels_ids)\n",
        "\n",
        "# 字典长度\n",
        "dict_size = len(dict_words)\n",
        "print(\"字典的长度\", dict_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "321533it [00:01, 205511.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "字典的长度 5158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gQ8SHdOjFSB2",
        "colab_type": "code",
        "outputId": "fa0814ce-c877-447b-d3c7-4ff7361593e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "maxLen = 32\n",
        "\n",
        "# 把句子中的文字转换成id，固定长度为32，不足32的部分用0补齐\n",
        "def sentence_padding(sentence):\n",
        "  sentence_ids = list(word2id[sentence])\n",
        "  if len(sentence_ids) > maxLen:\n",
        "    return sentence_ids[: maxLen]\n",
        "  sentence_ids.extend( [0] * (maxLen - len(sentence_ids)) )\n",
        "  return sentence_ids\n",
        "\n",
        "\n",
        "# 把文字对应的标记转换成id，固定长度为32，不足32的部分用标记x对应的0补齐\n",
        "def label_padding(labels):\n",
        "  label_ids = list(label2id[labels])\n",
        "  if len(label_ids) > maxLen:\n",
        "    return label_ids[: maxLen]\n",
        "  label_ids.extend( [0] * (maxLen - len(label_ids)) )\n",
        "  return label_ids\n",
        "  \n",
        "\n",
        "%time df_data['X'] = df_data[\"sentence_words\"].apply(sentence_padding)\n",
        "%time df_data['Y'] = df_data[\"sentence_labels\"].apply(label_padding)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 40s, sys: 5.62 s, total: 2min 45s\n",
            "Wall time: 2min 37s\n",
            "CPU times: user 2min 40s, sys: 5.72 s, total: 2min 45s\n",
            "Wall time: 2min 37s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0q8qCESRFKv5",
        "colab_type": "code",
        "outputId": "5dcf58b1-e823-4b34-f3de-e2f4634075ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.asarray(list( df_data['X'].values ))\n",
        "Y = np.asarray(list( df_data['Y'].values ))\n",
        "\n",
        "print(\"X.shape\", X.shape)\n",
        "print(\"Y.shape\", Y.shape)\n",
        "print(\"X样例\",X[0])\n",
        "print(\"Y样例\",Y[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape (321533, 32)\n",
            "Y.shape (321533, 32)\n",
            "X样例 [  8  43 320  88  36 198   7   2  41 163 124 245   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Y样例 [2 4 1 1 2 4 1 1 1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ak_zvUBifFu9",
        "colab_type": "code",
        "outputId": "ad2aa538-09d8-44c8-aac6-ee196092d884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "# 保存处理好的数据\n",
        "\n",
        "if not os.path.exists('data/'):\n",
        "  os.makedirs('data/')\n",
        "\n",
        "with open('data/data.pkl', 'wb') as output:\n",
        "  %time pickle.dump(X, output)\n",
        "  %time pickle.dump(Y, output)\n",
        "  pickle.dump(word2id, output)\n",
        "  pickle.dump(id2word, output)\n",
        "  pickle.dump(label2id, output)\n",
        "  pickle.dump(id2label, output)\n",
        "print(\"数据已保存至data/data.pkl\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 37.7 ms, sys: 129 ms, total: 167 ms\n",
            "Wall time: 167 ms\n",
            "CPU times: user 32.6 ms, sys: 94.2 ms, total: 127 ms\n",
            "Wall time: 190 ms\n",
            "数据已保存至data/data.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jNhaaFe-g6M2",
        "colab_type": "code",
        "outputId": "8258be4b-63a2-4f98-dfbe-8e8adae2d899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# 导入数据\n",
        "import pickle\n",
        "\n",
        "with open('data/data.pkl', 'rb') as _input:\n",
        "  X = pickle.load(_input)\n",
        "  Y = pickle.load(_input)\n",
        "  word2id = pickle.load(_input)\n",
        "  id2word = pickle.load(_input)\n",
        "  label2id = pickle.load(_input)\n",
        "  id2label = pickle.load(_input)\n",
        "\n",
        "\n",
        "\n",
        "# 划分测试集/训练集/验证集\n",
        "dataLen = len(X)\n",
        "testNum = int(dataLen * 0.2)\n",
        "X_test = X[dataLen - testNum:dataLen, :]\n",
        "Y_test = Y[dataLen - testNum:dataLen, :]\n",
        "\n",
        "\n",
        "X_train_valid = X[0:dataLen - testNum, :]\n",
        "Len = len(X_train_valid)\n",
        "validNum = int(Len * 0.2)\n",
        "X_valid = X[Len - validNum: Len,:]\n",
        "Y_valid = Y[Len - validNum: Len,:]\n",
        "\n",
        "X_train = X[:Len - validNum,:]\n",
        "Y_train = Y[:Len - validNum,:]\n",
        "print(X_test.shape)\n",
        "print(X_valid.shape)\n",
        "print(X_train.shape)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64306, 32)\n",
            "(51445, 32)\n",
            "(205782, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1MJ86FWL1oE4",
        "colab_type": "code",
        "outputId": "abff69b0-97ee-4052-8792-d12a0684e91c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 重置计算图，避免变量重复\n",
        "tf.reset_default_graph()\n",
        "\n",
        "decay = 0.85\n",
        "max_epoch = 5\n",
        "max_max_epoch = 10\n",
        "\n",
        "timestep_size = maxLen = 32\n",
        "# 样本中不同字的个数+1(padding 0)，根据处理数据的时候得到\n",
        "dict_size = 5158 + 1\n",
        "\n",
        "input_size = embedding_size = 64\n",
        "class_num = 5\n",
        "hidden_size = 128\n",
        "# bi-lstm 层数\n",
        "layer_num = 2\n",
        "max_grad_norm = 5.0\n",
        "model_save_path = 'ckpt/bi-lstm.ckpt'\n",
        "\n",
        "\n",
        "lr = tf.placeholder(tf.float32, name=\"lr\")\n",
        "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
        "batch_size = tf.placeholder(tf.int32, name=\"batch_size\")\n",
        "\n",
        "embedding = tf.get_variable(\"embedding\", [dict_size, embedding_size], dtype=tf.float32)\n",
        "\n",
        "def lstm_cell():\n",
        "  cell = rnn.LSTMCell(hidden_size)\n",
        "  cell = rnn.DropoutWrapper(cell, output_keep_prob = keep_prob)\n",
        "  return cell\n",
        "\n",
        "def bi_lstm(X):\n",
        "  # [batchsize, timestep_size] => [batchsize, timestep_size, embedding_size]\n",
        "  inputs = tf.nn.embedding_lookup(embedding, X)\n",
        "  # print(inputs.shape) # (?, 32, 64)\n",
        "  \n",
        "  # 2层前向lstm\n",
        "  cell_fw = rnn.MultiRNNCell( [lstm_cell() for _ in range(layer_num)], state_is_tuple = True )\n",
        "  # 2层反向lstm\n",
        "  cell_bw = rnn.MultiRNNCell( [lstm_cell() for _ in range(layer_num)], state_is_tuple = True )\n",
        "  \n",
        "  \n",
        "  # 将inputs转换为static_bidirectional_rnn要求的形式\n",
        "  # 即，把inputs.shape = [batchsize, timestep_size, embedding_size] 转为：\n",
        "  # timestep_size 个张量, 每个张量的shape = [batchsize, embedding_size]\n",
        "  inputs = tf.unstack(inputs, timestep_size, 1)\n",
        "  \n",
        "  # 由 timestep_num=32 个 shape 为 [batch_size, hidden_size] 的张量构成的列表\n",
        "  # 32 个时间步，每个时间步都输入了batch_size个数据，并且每个数据都输出hidden_size大小的输出\n",
        "  try:\n",
        "    outputs,_,_ = tf.contrib.rnn.static_bidirectional_rnn(cell_fw, cell_bw, inputs, dtype=tf.float32)\n",
        "  except Exception:\n",
        "    # 旧版tensorflow的返回\n",
        "    outputs = tf.contrib.rnn.static_bidirectional_rnn(cell_fw, cell_bw, inputs, dtype=tf.float32)\n",
        "  # 32，32个时间步\n",
        "  # print(len(outputs))  \n",
        "  \n",
        "  # 注意，bi-lstm中正向有128个输出，反向有128个输出，最终两个输出会连接在一起:256=128+128\n",
        "  # print(outputs[0].shape)  # (batch_size, 256) \n",
        "  # print(outputs[1].shape)  # (batch_size, 256) \n",
        "  # 这是最后一个时间步的输出\n",
        "  # print(outputs[-1].shape)  # (batch_size, 256) \n",
        "  # ---------------------------------------------------------------------\n",
        "  # 将timestep_num个维度为[batch_size, hidden_size]的矩阵按hidden_size维度拼接\n",
        "  # 得到的矩阵维度：[batch_size, hidden_size * timestep_num]\n",
        "  output = tf.concat(outputs, 1) # shape (64306, 32*256 = 8192)\n",
        "  #  将outputs转换成 [T, hidden_size * 2]\n",
        "  output = tf.reshape(output, [-1, hidden_size * 2]) \n",
        "  return output\n",
        "\n",
        "X_input = tf.placeholder(tf.int32, [None, timestep_size], name=\"X_input\")\n",
        "Y_input = tf.placeholder(tf.int32, [None, timestep_size], name=\"Y_input\")\n",
        "\n",
        "bilstm_output = bi_lstm(X_input)\n",
        "\n",
        "softmax_w = tf.Variable(tf.truncated_normal(shape = [hidden_size * 2, class_num], mean = 0, stddev = 0.5))\n",
        "softmax_b = tf.Variable(tf.truncated_normal(shape = [class_num], mean = 0, stddev = 0.5))\n",
        "\n",
        "y_pred = tf.add( tf.matmul(bilstm_output, softmax_w), softmax_b, name=\"y_pred\" )\n",
        "\n",
        "# 注意Y_input.shape = [batch_size, timestep_size]，需要“拉直”\n",
        "correct_prediction = tf.equal(tf.cast(tf.argmax(y_pred, 1), tf.int32), tf.reshape(Y_input, [-1]))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
        "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = tf.reshape(Y_input, [-1]), logits = y_pred), name=\"cost\")\n",
        "\n",
        "\n",
        "# ***** 梯度裁剪 *******\n",
        "tvars = tf.trainable_variables()  # 获取模型的所有参数\n",
        "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)  # 获取损失函数对于每个参数的梯度\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr)   # 优化器\n",
        "\n",
        "# 梯度下降计算\n",
        "train_op = optimizer.apply_gradients( zip(grads, tvars),\n",
        "    global_step=tf.contrib.framework.get_or_create_global_step())\n",
        "# print('Finished creating the bi-lstm model.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-11-98a362c17e45>:33: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-11-98a362c17e45>:43: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-11-98a362c17e45>:56: static_bidirectional_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell, unroll=True))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:1565: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From <ipython-input-11-98a362c17e45>:99: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "urY8Yi2rBpcd",
        "colab_type": "code",
        "outputId": "fb643d9f-1f38-4281-a212-9532513ab6d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "cell_type": "code",
      "source": [
        "# 将输入特征和label分割成batch_size大小\n",
        "def get_batch_X_Y(features, labels, batch_size):\n",
        "  for start in range(0, len(features), batch_size):\n",
        "    end = min(start + batch_size, len(features))\n",
        "    yield features[start:end], labels[start:end], int(start/batch_size)\n",
        "\n",
        "# 计算验证集的准确率\n",
        "def show_valid_acc(sess):\n",
        "  valid_batch_size = 512\n",
        "  valid_accuracy = 0.0\n",
        "  for valid_batch_x, valid_batch_y, valid_batch_index in get_batch_X_Y(X_valid, Y_valid, valid_batch_size):\n",
        "    valid_acc = sess.run(accuracy, \n",
        "              feed_dict={\n",
        "                X_input: valid_batch_x,\n",
        "                Y_input: valid_batch_y,\n",
        "                lr:1e-5,\n",
        "                batch_size:valid_batch_size,\n",
        "                keep_prob: 1.0\n",
        "              })\n",
        "    valid_accuracy += valid_acc\n",
        "  valid_accuracy /= int(len(Y_valid) / valid_batch_size)\n",
        "  return valid_accuracy\n",
        "\n",
        "# 开始训练\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  train_batch_size = 128\n",
        "  epoch_num = 6\n",
        "  # 每个 epoch 中包含的 batch 数\n",
        "  train_batch_num = int( len(X_train) / train_batch_size)  \n",
        "  # print(train_batch_num) # 1607\n",
        "  display_num = 50\n",
        "  display_batch =  int( len(X_train) / display_num) \n",
        "  saver = tf.train.Saver() \n",
        "\n",
        "  learning_rate = 1e-4\n",
        "  decay = 0.85\n",
        "  mid_epoch = 5\n",
        "  total_epoch = 6\n",
        "\n",
        "  for epoch in range(total_epoch):\n",
        "    if epoch > mid_epoch:\n",
        "      learning_rate = learning_rate * ( (decay) ** (epoch - mid_epoch) )\n",
        "    print(\"epoch %d :\" % (epoch + 1))\n",
        "    start_time = time.time()\n",
        "    train_loss = 0\n",
        "    valid_accr = 0\n",
        "    for batch_x, batch_y, batch_index in get_batch_X_Y(X_train, Y_train, train_batch_size):\n",
        "      feed_dict = {\n",
        "              X_input: batch_x,\n",
        "              Y_input: batch_y,\n",
        "              lr:learning_rate,\n",
        "              batch_size:train_batch_size,\n",
        "              keep_prob: 0.5\n",
        "            }\n",
        "      _, loss, accr = sess.run([train_op, cost, accuracy], feed_dict=feed_dict)\n",
        "      if (batch_index + 1) % 400 == 0:\n",
        "        print(\"   准确率 =\", show_valid_acc(sess), \"loss =\", loss)\n",
        "  save_path = saver.save(sess, model_save_path)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1 :\n",
            "   准确率 = 0.761001033782959 loss = 0.62942934\n",
            "   准确率 = 0.7805332505702972 loss = 0.36704862\n",
            "   准确率 = 0.7735305523872376 loss = 0.5307258\n",
            "   准确率 = 0.8031831902265548 loss = 0.40916482\n",
            "epoch 2 :\n",
            "   准确率 = 0.8464404124021531 loss = 0.45108005\n",
            "   准确率 = 0.8739796763658524 loss = 0.25456482\n",
            "   准确率 = 0.9029435861110687 loss = 0.3392535\n",
            "   准确率 = 0.9135383301973343 loss = 0.2291247\n",
            "epoch 3 :\n",
            "   准确率 = 0.9202643746137619 loss = 0.29349682\n",
            "   准确率 = 0.9231205457448959 loss = 0.20426863\n",
            "   准确率 = 0.9240581953525543 loss = 0.2872843\n",
            "   准确率 = 0.9291651558876037 loss = 0.19206083\n",
            "epoch 4 :\n",
            "   准确率 = 0.9341693586111068 loss = 0.24466524\n",
            "   准确率 = 0.9367320609092712 loss = 0.18867947\n",
            "   准确率 = 0.9381745254993439 loss = 0.24587512\n",
            "   准确率 = 0.9415771108865738 loss = 0.15257683\n",
            "epoch 5 :\n",
            "   准确率 = 0.9434335339069366 loss = 0.20684972\n",
            "   准确率 = 0.9439535462856292 loss = 0.17195609\n",
            "   准确率 = 0.9429237735271454 loss = 0.23020278\n",
            "   准确率 = 0.9455888968706131 loss = 0.1468554\n",
            "epoch 6 :\n",
            "   准确率 = 0.9465728384256363 loss = 0.1901618\n",
            "   准确率 = 0.94659215092659 loss = 0.16460882\n",
            "   准确率 = 0.9461402088403702 loss = 0.21726236\n",
            "   准确率 = 0.9479698085784912 loss = 0.1412093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WN2o8uPoiKBA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KyPo5LL7z-NV",
        "colab_type": "code",
        "outputId": "691cdc40-cc69-4469-fecf-9b5d4ce267b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# 重置计算图，避免变量重复\n",
        "tf.reset_default_graph()\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "\n",
        "save_model_path = \"ckpt/bi-lstm.ckpt\"\n",
        "\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "  loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
        "  loader.restore(sess, save_model_path)\n",
        "  \n",
        "  loaded_x = loaded_graph.get_tensor_by_name('X_input:0')\n",
        "  loaded_y = loaded_graph.get_tensor_by_name('Y_input:0')\n",
        "  loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "  loaded_lr = loaded_graph.get_tensor_by_name('lr:0')\n",
        "  loaded_batch_size = loaded_graph.get_tensor_by_name('batch_size:0')\n",
        "  loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
        "  \n",
        "  # 测试集合的准确率\n",
        "  test_batch_size = 512\n",
        "  test_accuracy = 0.0\n",
        "  for test_batch_x, test_batch_y, test_batch_index in get_batch_X_Y(X_test, Y_test, test_batch_size):\n",
        "    test_acc = sess.run(loaded_acc, \n",
        "              feed_dict={\n",
        "                loaded_x: test_batch_x,\n",
        "                loaded_y: test_batch_y,\n",
        "                loaded_lr:1e-5,\n",
        "                loaded_batch_size:test_batch_size,\n",
        "                loaded_keep_prob: 1.0\n",
        "              })\n",
        "    test_accuracy += test_acc\n",
        "  test_accuracy /= int(len(Y_test) / test_batch_size)\n",
        "  print(\"测试集准确率 =\", test_accuracy)\n",
        "# sess.close()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ckpt/bi-lstm.ckpt\n",
            "测试集准确率 = 0.9486892585754394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_3QNAKTHA-bz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bclAha-jS8Bi",
        "colab_type": "code",
        "outputId": "f268e69e-6642-45ed-ac5e-13bbdf4cd8ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "# A统计状态转移的频数\n",
        "A = {\n",
        "      'sb':0,\n",
        "      'ss':0,\n",
        "      'be':0,\n",
        "      'bm':0,\n",
        "      'me':0,\n",
        "      'mm':0,\n",
        "      'eb':0,\n",
        "      'es':0\n",
        "     }\n",
        "# zy 表示转移概率矩阵\n",
        "zy = {}\n",
        "for label in sentence_labels:\n",
        "    for t in range(len(label) - 1):\n",
        "        key = label[t] + label[t+1]\n",
        "        A[key] += 1.0\n",
        "        \n",
        "zy['sb'] = A['sb'] / (A['sb'] + A['ss'])\n",
        "zy['ss'] = 1.0 - zy['sb']\n",
        "zy['be'] = A['be'] / (A['be'] + A['bm'])\n",
        "zy['bm'] = 1.0 - zy['be']\n",
        "zy['me'] = A['me'] / (A['me'] + A['mm'])\n",
        "zy['mm'] = 1.0 - zy['me']\n",
        "zy['eb'] = A['eb'] / (A['eb'] + A['es'])\n",
        "zy['es'] = 1.0 - zy['eb']\n",
        "keys = sorted(zy.keys())\n",
        "print('the transition probability: ')\n",
        "for key in keys:\n",
        "    print(key, zy[key])\n",
        "\n",
        "\n",
        "zy = {i:np.log(zy[i]) for i in zy.keys()}\n",
        "with open('data/data_zy.pkl', 'wb') as output:\n",
        "  pickle.dump(zy, output)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the transition probability: \n",
            "be 0.8287395142819345\n",
            "bm 0.1712604857180655\n",
            "eb 0.5923696618295927\n",
            "es 0.4076303381704073\n",
            "me 0.5048718297888326\n",
            "mm 0.4951281702111674\n",
            "sb 0.6232520322915978\n",
            "ss 0.37674796770840224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1jW3k_Tni67E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "4c782d5d-e5cc-4867-8349-f3effd165feb"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "with open('data/data.pkl', 'rb') as _input:\n",
        "  X = pickle.load(_input)\n",
        "  Y = pickle.load(_input)\n",
        "  word2id = pickle.load(_input)\n",
        "  id2word = pickle.load(_input)\n",
        "  label2id = pickle.load(_input)\n",
        "  id2label = pickle.load(_input)\n",
        "\n",
        "\n",
        "with open('data/data_zy.pkl', 'rb') as _input:\n",
        "  zy = pickle.load(_input)\n",
        "\n",
        "maxLen = 32\n",
        "\n",
        "# 重置计算图，避免变量重复\n",
        "tf.reset_default_graph()\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "\n",
        "save_model_path = \"ckpt/bi-lstm.ckpt\"\n",
        "\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "  loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
        "  loader.restore(sess, save_model_path)\n",
        "  \n",
        "  loaded_x = loaded_graph.get_tensor_by_name('X_input:0')\n",
        "  loaded_y = loaded_graph.get_tensor_by_name('Y_input:0')\n",
        "  loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "  loaded_lr = loaded_graph.get_tensor_by_name('lr:0')\n",
        "  loaded_batch_size = loaded_graph.get_tensor_by_name('batch_size:0')\n",
        "  loaded_y_pred = loaded_graph.get_tensor_by_name('y_pred:0')\n",
        "  \n",
        "  \n",
        "  def viterbi(nodes):\n",
        "      \"\"\"\n",
        "      维特比译码：除了第一层以外，每一层有4个节点。\n",
        "      计算当前层（第一层不需要计算）四个节点的最短路径：\n",
        "         对于本层的每一个节点，计算出路径来自上一层的各个节点的新的路径长度（概率）。保留最大值（最短路径）。\n",
        "         上一层每个节点的路径保存在 paths 中。计算本层的时候，先用paths_ 暂存，然后把本层的最大路径保存到 paths 中。\n",
        "         paths 采用字典的形式保存（路径：路径长度）。\n",
        "         一直计算到最后一层，得到四条路径，将长度最短（概率值最大的路径返回）\n",
        "      \"\"\"\n",
        "      paths = {'b': nodes[0]['b'], 's':nodes[0]['s']} # 第一层，只有两个节点\n",
        "      for layer in range(1, len(nodes)):  # 后面的每一层\n",
        "          paths_ = paths.copy()  # 先保存上一层的路径\n",
        "          # node_now 为本层节点， node_last 为上层节点\n",
        "          paths = {}  # 清空 path \n",
        "          for node_now in nodes[layer].keys():\n",
        "              # 对于本层的每个节点，找出最短路径\n",
        "              sub_paths = {} \n",
        "              # 上一层的每个节点到本层节点的连接\n",
        "              for path_last in paths_.keys():\n",
        "                  if path_last[-1] + node_now in zy.keys(): # 若转移概率不为 0 \n",
        "                      sub_paths[path_last + node_now] = paths_[path_last] + nodes[layer][node_now] + zy[path_last[-1] + node_now]\n",
        "              # 最短路径,即概率最大的那个\n",
        "              sr_subpaths = pd.Series(sub_paths)\n",
        "              sr_subpaths = sr_subpaths.sort_values()  # 升序排序\n",
        "              node_subpath = sr_subpaths.index[-1]  # 最短路径\n",
        "              node_value = sr_subpaths[-1]   # 最短路径对应的值\n",
        "              # 把 node_now 的最短路径添加到 paths 中\n",
        "              paths[node_subpath] = node_value\n",
        "      # 所有层求完后，找出最后一层中各个节点的路径最短的路径\n",
        "      sr_paths = pd.Series(paths)\n",
        "      sr_paths = sr_paths.sort_values()  # 按照升序排序\n",
        "      return sr_paths.index[-1]  # 返回最短路径（概率值最大的路径）\n",
        "\n",
        "\n",
        "  def text2ids(text):\n",
        "      \"\"\"把字片段text转为 ids.\"\"\"\n",
        "      words = list(text)\n",
        "      ids = list(word2id[words])\n",
        "      if len(ids) >= maxLen:  # 长则弃掉\n",
        "          print(u'输出片段超过%d部分无法处理' % (maxLen)) \n",
        "          return ids[:maxLen]\n",
        "      ids.extend([0]*(maxLen-len(ids))) # 短则补全\n",
        "      ids = np.asarray(ids).reshape([-1, maxLen])\n",
        "      return ids\n",
        "\n",
        "\n",
        "  def simple_cut(text):\n",
        "      \"\"\"对一个片段text（标点符号把句子划分为多个片段）进行预测。\"\"\"\n",
        "      if text:\n",
        "          text_len = len(text)\n",
        "          X_batch = text2ids(text)  # 这里每个 batch 是一个样本\n",
        "          fetches = [loaded_y_pred]\n",
        "          feed_dict = {loaded_x:X_batch, loaded_lr:1.0, loaded_batch_size:1, loaded_keep_prob:1.0}\n",
        "          _y_pred = sess.run(fetches, feed_dict)[0][:text_len]  # padding填充的部分直接丢弃\n",
        "          nodes = [dict(zip(['s','b','m','e'], each[1:])) for each in _y_pred]\n",
        "          tags = viterbi(nodes)\n",
        "          words = []\n",
        "          for i in range(len(text)):\n",
        "              if tags[i] in ['s', 'b']:\n",
        "                  words.append(text[i])\n",
        "              else:\n",
        "                  words[-1] += text[i]\n",
        "          return words\n",
        "      else:\n",
        "          return []\n",
        "\n",
        "\n",
        "  def cut_word(sentence):\n",
        "      \"\"\"首先将一个sentence根据标点和英文符号/字符串划分成多个片段text，然后对每一个片段分词。\"\"\"\n",
        "      not_cuts = re.compile('([0-9\\da-zA-Z ]+)|[。，、？！.\\.\\?,!]')\n",
        "      result = []\n",
        "      start = 0\n",
        "      for seg_sign in not_cuts.finditer(sentence):\n",
        "          result.extend(simple_cut(sentence[start:seg_sign.start()]))\n",
        "          result.append(sentence[seg_sign.start():seg_sign.end()])\n",
        "          start = seg_sign.end()\n",
        "      result.extend(simple_cut(sentence[start:]))\n",
        "      return result\n",
        "  # 例子\n",
        "  sentence = '新京报讯 （记者 滕朝）2月17日，由郭帆导演，吴京特别出演，屈楚萧、李光洁、吴孟达、赵今麦主演的《流浪地球》票房达到36.51亿（含预售票房），超过《红海行动》的36.50亿，成为目前内地影史票房亚军，正在向内地影史票房冠军《战狼2》的56.8亿目标冲刺。至此，演员吴京的作品包揽了内地票房榜的前两位。'\n",
        "  result = cut_word(sentence)\n",
        "  rss = ''\n",
        "  for each in result:\n",
        "      rss = rss + each + ' / '\n",
        "  print(rss)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ckpt/bi-lstm.ckpt\n",
            "新京 / 报讯 /   / （ / 记者 /   / 滕朝 / ） / 2 / 月 / 17 / 日 / ， / 由 / 郭帆导演 / ， / 吴京 / 特别 / 出演 / ， / 屈楚 / 萧 / 、 / 李光洁 / 、 / 吴孟达 / 、 / 赵今麦 / 主演 / 的 / 《 / 流浪 / 地球 / 》 / 票房 / 达到 / 36 / . / 51 / 亿（ / 含 / 预售 / 票房 / ） / ， / 超过 / 《 / 红海 / 行动 / 》 / 的 / 36 / . / 50 / 亿 / ， / 成为 / 目前 / 内地 / 影史 / 票房 / 亚军 / ， / 正在 / 向 / 内地 / 影史 / 票房 / 冠军 / 《 / 战狼 / 2 / 》 / 的 / 56 / . / 8 / 亿目标 / 冲刺 / 。 / 至此 / ， / 演员 / 吴京 / 的 / 作品 / 包揽 / 了 / 内 / 地票 / 房榜 / 的 / 前 / 两位 / 。 / \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zId5xASi9-iv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "参考链接：\n",
        "1. http://spaces.ac.cn/archives/3924/ \n",
        "2. https://github.com/yongyehuang/deepnlp/blob/master/deepnlp/pos/pos_model_bilstm.py\n",
        "3. https://blog.csdn.net/Jerr__y/article/details/70471066"
      ]
    },
    {
      "metadata": {
        "id": "-KmjeUrt-omj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}